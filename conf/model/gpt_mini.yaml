
# Training parameters
eval_only: False
save_checkpoint: True
init_from: 'scratch'
batch_size: 12
block_size: 1024
accumulation_steps: 5 * 8

# adamw optimizer
learning_rate: 6e-4
max_iters: 600000
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

# Model parameters
model_config:
  _target_: src.config.gpt_config.GPTConfig
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.0
  bias: False

# Learning rate decay settings
decay_lr: True
warmup_iters: 2000
lr_decay_iters: 600000
min_lr: 6e-5