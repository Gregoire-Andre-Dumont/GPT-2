defaults:
  - wandb
  - model/gpt_mini

# training and validation
train_path: data/train.parquet
val_path: data/validation.parquet


eval_interval: 2000
log_interval: 1
eval_iters: 200
eval_only: False
save_checkpoint: True
init_from: 'scratch'

# Training parameters
batch_size: 12
block_size: 1024
accumulation_steps: 5 * 8

# adamw optimizer
learning_rate: 6e-4
max_iters: 600000
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

# Model parameters
n_layer: 12
n_head: 12
n_embd: 768
dropout: 0.0
bias: False

# Learning rate decay settings
decay_lr: True
warmup_iters: 2000
lr_decay_iters: 600000
min_lr: 6e-5

# DDP settings
backend: 'nccl' # 'nccl', 'gloo', etc.