{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T11:52:59.648889Z",
     "start_time": "2025-03-27T11:52:54.832220Z"
    }
   },
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load BERT for MLM\n",
    "bert_model = BertForMaskedLM.from_pretrained(\"bert-large-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dumon\\PycharmProjects\\GPT-2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:52:59.664285Z",
     "start_time": "2025-03-27T11:52:59.658468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_augment(text: str, p_bert: float) -> str:\n",
    "    \"\"\"Data augmentation with mask language modeling.\"\"\"\n",
    "\n",
    "    # Extract the subwords tokens from the text\n",
    "    tokenized_text = np.array(text)\n",
    "\n",
    "    # Randomly mask tokens in the text\n",
    "    mask = np.random.rand(len(tokenized_text)) < p_bert\n",
    "    masked_text = np.where(mask, \"[MASK]\", tokenized_text).tolist()\n",
    "    mask_indices = np.where(mask)[0].tolist()\n",
    "\n",
    "    # Convert to BERT input format\n",
    "    indexed_tokens = bert_tokenizer.convert_tokens_to_ids(masked_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    # Predict masked tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    # Extract predicted token indices for all masked positions at once\n",
    "    predicted_indices = torch.argmax(predictions[0, mask_indices], dim=1).tolist()\n",
    "    predicted_tokens = bert_tokenizer.convert_ids_to_tokens(predicted_indices)\n",
    "\n",
    "    # Replace MASK tokens with predictions\n",
    "    augmented_tokens = tokenized_text.copy()\n",
    "    for idx, mask_pos in enumerate(mask_indices):\n",
    "        augmented_tokens[mask_pos] = predicted_tokens[idx]\n",
    "\n",
    "    # Convert back to text\n",
    "    return bert_tokenizer.convert_tokens_to_string(augmented_tokens)"
   ],
   "id": "c4b34481e6c6b8b6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:53:00.283072Z",
     "start_time": "2025-03-27T11:52:59.679204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "texts = pd.read_parquet('../data/text8/test.parquet')['text']\n",
    "text = texts[0][:1000]\n",
    "augmented_text = data_augment(text, p_bert=0.2)\n",
    "\n",
    "print(augmented_text)"
   ],
   "id": "33dc772caa5ca933",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m texts \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/text8/test.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      3\u001B[0m text \u001B[38;5;241m=\u001B[39m texts[\u001B[38;5;241m0\u001B[39m][:\u001B[38;5;241m1000\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m augmented_text \u001B[38;5;241m=\u001B[39m \u001B[43mdata_augment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp_bert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(augmented_text)\n",
      "Cell \u001B[1;32mIn[2], line 10\u001B[0m, in \u001B[0;36mdata_augment\u001B[1;34m(text, p_bert)\u001B[0m\n\u001B[0;32m      7\u001B[0m tokenized_text \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(text)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Randomly mask tokens in the text\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokenized_text\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m<\u001B[39m p_bert\n\u001B[0;32m     11\u001B[0m masked_text \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(mask, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[MASK]\u001B[39m\u001B[38;5;124m\"\u001B[39m, tokenized_text)\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m     12\u001B[0m mask_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(mask)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n",
      "\u001B[1;31mTypeError\u001B[0m: len() of unsized object"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:55:09.487742Z",
     "start_time": "2025-03-27T11:55:09.269420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ],
   "id": "d6776be5f381b6fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:02:53.151780Z",
     "start_time": "2025-03-27T12:02:51.919338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\".*indexing errors.*\")\n",
    "\n",
    "tokenized_text = tokenizer.encode(texts[0][:10000], add_special_tokens=True)\n",
    "tokenized_text = tokenized_text[:512]\n",
    "\n",
    "text = tokenizer.decode(tokenized_text)\n",
    "tokenized_text = bert_tokenizer.tokenize(text)\n",
    "\n",
    "augmented = data_augment(tokenized_text[:512], p_bert=0.2)\n",
    "augmented = tokenizer.encode(augmented,truncation=True, max_length=512)\n",
    "\n",
    "input = augmented[:-1] # pad to 512\n",
    "target = augmented[1:] # pad to 512"
   ],
   "id": "4028e0df54f77f48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T12:02:54.872932Z",
     "start_time": "2025-03-27T12:02:54.853400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = augmented[:-1]\n",
    "target = augmented[1:]"
   ],
   "id": "b8dee7ea4e499b24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82ac8e341f4ed450"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
