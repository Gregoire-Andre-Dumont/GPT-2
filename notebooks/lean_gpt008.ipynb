{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CyEiETbQeBr",
    "outputId": "a0171d44-f7b8-482c-8d0b-c5fc937a2b96"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!pip install datasets\n",
    "!pip install tiktoken\n",
    "import pickle\n",
    "import os\n",
    "import tiktoken\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZY0g4cLw15sS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install torchtune\n",
    "# !pip install torch torchvision torchao"
   ],
   "metadata": {
    "id": "yMVel_lm4o1o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rotary-embedding-torch\n",
    "!pip install torchao\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from typing import final\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzvfZ6tg8GYs",
    "outputId": "4318bbae-bd6c-4244-854c-f0da29a1a203"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.11/dist-packages (0.8.6)\n",
      "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.11/dist-packages (from rotary-embedding-torch) (0.8.1)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from rotary-embedding-torch) (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->rotary-embedding-torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->rotary-embedding-torch) (3.0.2)\n",
      "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super().__init__()\n",
    "        self.combined_linear = nn.Linear(dimension, 2 * dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        combined = self.combined_linear(x)\n",
    "        linear_1, linear_2 = combined.chunk(2, dim=-1)\n",
    "        swish = linear_1 * F.relu(linear_1)\n",
    "        return swish * linear_2"
   ],
   "metadata": {
    "id": "HzQ8NUeqfrq6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchtune\n",
    "!pip install torch torchvision torchao"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "459PUdhr8sDG",
    "outputId": "2c9adfcc-c493-4628-e184-2a516cb50ffe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.4.1)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.29.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.10)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
      "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.22.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (2.3.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
      "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.14)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchtune"
   ],
   "metadata": {
    "id": "g-x6KtYd9xNT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class groupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "      super().__init__()\n",
    "      self.config = config\n",
    "      self.kv_factor = config.n_head//config.kv_heads\n",
    "      self.n_embd = config.n_embd\n",
    "      self.n_head = config.n_head\n",
    "      self.kv_head = config.kv_heads\n",
    "      self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=config.bias)\n",
    "      self.dropout = config.dropout\n",
    "      self.init_weight_normalization_flag = True\n",
    "      self.q_proj = nn.Linear(self.n_embd, self.n_embd, bias=config.bias)\n",
    "      self.k_proj = nn.Linear(self.n_embd, self.n_embd//(self.kv_factor), bias=config.bias)\n",
    "      self.v_proj = nn.Linear(self.n_embd, self.n_embd//(self.kv_factor), bias=config.bias)\n",
    "      self.rotary_emb = RotaryEmbedding(dim = self.n_embd // self.n_head)\n",
    "      self.KVcache = torchtune.modules.KVCache(batch_size=config.batch_size,max_seq_len=config.block_size,\n",
    "                                               num_kv_heads=config.kv_heads,head_dim=self.n_embd//(self.kv_factor),dtype=torch.bfloat16)\n",
    "      self.max_sequence_length = 4096\n",
    "      self.attn_gqa = torchtune.modules.MultiHeadAttention(embed_dim=self.n_head,num_heads=self.n_head,num_kv_heads=self.kv_head,\n",
    "                                                            head_dim=self.n_embd//self.n_head,\n",
    "                                                            q_proj=self.q_proj,k_proj=self.k_proj,v_proj=self.v_proj,\n",
    "                                                            output_proj=self.c_proj,\n",
    "                                                            attn_dropout=config.dropout,kv_cache=self.KVcache)\n",
    "    def forward(self, x):\n",
    "      B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "      y = self.attn_gqa(x,x,)\n",
    "      y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "      # output projection\n",
    "      return y"
   ],
   "metadata": {
    "id": "TiRgiSfeIZnC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        #Prevent standart deviatin creep.\n",
    "        self.init_weight_normalization_flag = True\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        self.kv_head = 6\n",
    "        # self.gqa = True\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "        self.rotary_emb = RotaryEmbedding(dim = self.n_embd // self.n_head)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        query, key, value  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        key = key.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        query = query.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        value = value.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # query = self.rotary_pe(query) # Changed this line\n",
    "        # key = self.rotary_pe(key) # Changed this line\n",
    "\n",
    "        query = self.rotary_emb.rotate_queries_or_keys(query)\n",
    "        key = self.rotary_emb.rotate_queries_or_keys(key)\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True,enable_gqa = True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (query @ key.transpose(-2, -1)) * (1.0 / math.sqrt(key.size(-1)))\n",
    "            # only causal attention\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            #normalization\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            #weighted sum of \"intersting\" tokens\n",
    "            y = att @ value # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "O8IjUqiMPA0o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eo1OANCSQM4A"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        if config.swiglu:\n",
    "          self.swiglu  = SwiGLU(4 * config.n_embd)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        if config.swiglu:\n",
    "          x = self.swiglu(x)\n",
    "        else:\n",
    "          x = torch.relu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        if config.gqa:\n",
    "          self.attn = groupedQueryAttention(config)\n",
    "        else:\n",
    "          self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if module.bias is None:\n",
    "              if hasattr(module,'init_weight_normalization_flag'):\n",
    "                  std = self.config.n_embd**-0.5\n",
    "              else:\n",
    "                  std = config.wte_std\n",
    "              torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.wte_std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.wpe_std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas,eps, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas,eps=eps, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjOgC5FMQ0kI"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BinaryTokenDataset(Dataset):\n",
    "    def __init__(self, bin_file_path, block_size):\n",
    "        # Read binary data directly into numpy array\n",
    "        self.data = np.fromfile(bin_file_path, dtype=np.uint16)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of possible blocks in the dataset\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get input sequence and target sequence\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "\n",
    "        # Convert to PyTorch tensors with appropriate dtype\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "st6xDf4GY3py"
   },
   "outputs": [],
   "source": [
    "def process_data_tiktokenizer(data):\n",
    "  encoding = tiktoken.get_encoding('gpt2')\n",
    "  tokens = encoding.encode(data)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KImVds3sZije"
   },
   "outputs": [],
   "source": [
    "class DataLoader:  # Renamed data_loader to DataLoader for better readability\n",
    "    def __init__(self, config,data_dir):\n",
    "        self.config = config\n",
    "        print(f\"batch size {config.batch_size}\")\n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.data_dir = data_dir\n",
    "        print(\"processing_data\")\n",
    "        self.current_position = 0\n",
    "\n",
    "    def get_batch(self,split='none'):\n",
    "      # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "      # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "      if split == 'train' or split == 'none':\n",
    "          data = np.memmap(os.path.join(self.data_dir, 'train_data.bin'), dtype=np.uint16, mode='r')\n",
    "      else:\n",
    "          data = np.memmap(os.path.join(self.data_dir, 'val_data.bin'), dtype=np.uint16, mode='r')\n",
    "      ix = torch.randint(len(data) - block_size, (self.batch_size,))\n",
    "      x = torch.stack([torch.from_numpy((data[i:i+self.block_size]).astype(np.int64)) for i in ix])\n",
    "      y = torch.stack([torch.from_numpy((data[i+1:i+1+self.block_size]).astype(np.int64)) for i in ix])\n",
    "      if device == 'cuda':\n",
    "          # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "          x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "      else:\n",
    "          x, y = x.to(device), y.to(device)\n",
    "      return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqincBWthrLr"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tiktoken\n",
    "import struct\n",
    "def get_tokenizer(encoding_name=\"gpt2\"):\n",
    "    # Initialize the tokenizer with the specified encoding\n",
    "    tokenizer = tiktoken.get_encoding(encoding_name)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(tokenizer, text):\n",
    "    # Encode the text into token IDs\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "def decode_tokens(tokenizer, token_ids):\n",
    "    # Decode the token IDs back into text\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "def encode_and_save_tokens(file_path, output_file_path, encoding_name=\"gpt2\"):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(encoding_name)\n",
    "\n",
    "    # Read the file content\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Encode the text into token IDs\n",
    "    token_ids = tokenizer.encode(text)\n",
    "\n",
    "    # Convert to numpy array with uint16 data type and save\n",
    "    token_ids_array = np.array(token_ids, dtype=np.uint16)\n",
    "    token_ids_array.tofile(output_file_path)\n",
    "    return token_ids_array.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB93MdIhJzgE"
   },
   "outputs": [],
   "source": [
    "def learning_rate_scheduler(it,config):\n",
    "    if it < config.warmup_steps:\n",
    "        lr = config.max_lr * (it + 1) / config.warmup_steps\n",
    "    elif it >= config.max_iters - config.warmup_steps:\n",
    "        lr = config.min_lr\n",
    "    else:\n",
    "        decay_ratio = (it - config.warmup_steps) / (config.max_iters - config.warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        k = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # linear\n",
    "        lr = config.learning_rate * k\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "hUEfZtJCGvBt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCV9SLQmTCdi"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader,scaler,config,LOG=False,SCALING=False):\n",
    "    if LOG:\n",
    "      logger = initWanDb()\n",
    "    else:\n",
    "      logger = 0;\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    grad_accum_steps = config.total_batch_size//(config.batch_size*config.block_size)\n",
    "    model.train()\n",
    "    epoch = 0\n",
    "    execution_time = 0\n",
    "    for steps in range(config.max_iters):\n",
    "      loss = torch.tensor(0.0, device=device)\n",
    "      loss_accum = 0\n",
    "      start_time = time.time()\n",
    "      for micro_step in range(grad_accum_steps):\n",
    "\n",
    "        xb,yb = data_loader.get_batch(split='train')\n",
    "        #evaluate loss\n",
    "        #use mixed precision\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "          logits, loss = model(xb, yb)\n",
    "        loss = loss/grad_accum_steps\n",
    "        loss_accum +=loss.detach()\n",
    "        #backward pass\n",
    "        scaler.scale(loss).backward() if SCALING else loss.backward()\n",
    "\n",
    "      if SCALING:\n",
    "        scaler.unscale_(optimizer)\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "      else:\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip_norm)\n",
    "        optimizer.step()\\\n",
    "\n",
    "\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "      learning_rate = learning_rate_scheduler(steps,config)\n",
    "      optimizer.param_groups[0]['lr'] = learning_rate\n",
    "      end_time = time.time()\n",
    "      dt = end_time - start_time\n",
    "      execution_time+=dt\n",
    "      mfu = model.estimate_mfu(config.batch_size, dt)\n",
    "      if (steps%10 == 0 or steps == 0):\n",
    "        print(f\"|step {steps}| avg loss: {grad_accum_steps*loss.item():.3f}| mfu: {mfu*100:.2f}%| time: {dt*1000:.2f}ms| norm: {norm.item():.3f} |lr: {learning_rate:.4e}| epoch: {epoch}\")\n",
    "        if LOG:\n",
    "          logger.log({\"loss\": loss_accum, \"lr\": learning_rate,\"proccess time\":execution_time})\n",
    "      if LOG:\n",
    "        if (steps%config.eval_iters == 0 and steps != 0):\n",
    "          print(\"_____________Evaluation_____________\")\n",
    "          with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.get_batch(split=\"val\")\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "          print(f\"validation loss: {val_loss_accum}\")\n",
    "          logger.log({\"validation loss\": val_loss_accum,\"epoch\": epoch})\n",
    "          checkpoint_path = f\"model_epoch_{epoch}.pth\"\n",
    "          torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'scaler_state_dict': scaler.state_dict(),\n",
    "              'config': config,\n",
    "          }, checkpoint_path)\n",
    "          wandb.save(checkpoint_path)  # Ensure wandb is imported in the scope\\\n",
    "\n",
    "          print(\"Generated line\")\n",
    "          for _ in range(5):\n",
    "              idx = model.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=50)[0].tolist()\n",
    "              print(decode_tokens(tokenizer, idx))\n",
    "          model.train()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    total_batch_size: int = 256*4\n",
    "    batch_size: int = 16\n",
    "    block_size: int = 128\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 192\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    gqa: bool = False\n",
    "    swiglu: bool = False\n",
    "    kv_heads: int = 4\n",
    "\n",
    "    max_iters: int = 3000\n",
    "    learning_rate: float = 6e-4\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    weight_decay: float = 1e-1\n",
    "    eval_iters: int = 200\n",
    "    wte_std: float = .02 # Javier innitilization\n",
    "    wpe_std: float = 0.02\n",
    "\n",
    "    grad_clip_norm: float = 1.0\n",
    "    learning_rate: float = 6e-4\n",
    "    max_lr: float = 3e-4\n",
    "    min_lr: float = max_lr*0.1\n",
    "    warmup_steps: int = 50\n",
    "    optimizer_alpha: float = .9\n",
    "    optimizer_beta: float = .95\n",
    "    optimizer_eps: float = 1e-8\n",
    "    optimizer_weight_decay: float = 1e-1\n",
    "    optimizer_eps: float = 1e-8\n",
    "\n",
    "    train_data_size: int = 0\n",
    "    val_data_size: int = 0"
   ],
   "metadata": {
    "id": "_64H4laxPMdr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d1Z0GM5QvNK"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import LiteralString\n",
    "total_batch_size: int = 64*(128)*4\n",
    "batch_size: int = 16\n",
    "block_size: int = 128\n",
    "vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "n_layer: int = 12\n",
    "n_head: int = 12\n",
    "n_embd: int = 192\n",
    "dropout: float = 0.2\n",
    "bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "eval_iters: int = 200\n",
    "max_iters: int = 1200\n",
    "wte_std: float = 1/np.sqrt(n_embd) # Javier innitilization\n",
    "wpe_std: float = 1/np.sqrt(n_embd)\n",
    "gqa: bool = False\n",
    "kv_heads: int = 4\n",
    "swiglu: bool = True\n",
    "\n",
    "learning_rate: float = 6e-4\n",
    "max_lr: float = 3e-4\n",
    "min_lr: float = max_lr*0.1\n",
    "warmup_steps: int = 50\n",
    "optimizer_alpha: float = .9\n",
    "optimizer_beta: float = .95\n",
    "optimizer_eps: float = 1e-8\n",
    "optimizer_weight_decay: float = 1e-1\n",
    "optimizer_eps: float = 1e-8\n",
    "grad_clip_norm: float = 1.0\n",
    "\n",
    "train_data_size: int = 0\n",
    "val_data_size: int = 0\n",
    "run_name = \"swiglu_05_dropout_lr_change\"\n",
    "\n",
    "\n",
    "config = GPTConfig(\n",
    "    total_batch_size = total_batch_size,\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    dropout=dropout,\n",
    "    bias=bias,\n",
    "    batch_size = batch_size,\n",
    "    max_iters = max_iters,\n",
    "    eval_iters = eval_iters,\n",
    "    wte_std = wte_std,\n",
    "    wpe_std = wpe_std,\n",
    "    learning_rate = learning_rate,\n",
    "    min_lr = min_lr,\n",
    "    max_lr = max_lr,\n",
    "    warmup_steps = warmup_steps,\n",
    "    optimizer_alpha = optimizer_alpha,\n",
    "    optimizer_beta = optimizer_beta,\n",
    "    optimizer_eps = optimizer_eps,\n",
    "    optimizer_weight_decay = optimizer_weight_decay,\n",
    "    grad_clip_norm = grad_clip_norm,\n",
    "    train_data_size = train_data_size,\n",
    "    val_data_size = val_data_size,\n",
    "    gqa = gqa,\n",
    "    kv_heads = kv_heads,\n",
    "    swiglu = swiglu\n",
    ") # Change to GPTConfig object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tne_g9uQ22L_",
    "outputId": "9bd759dd-ac39-4505-b473-0d117e19cfa3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "# use name=\"sample-10BT\" to use the 10BT sample\n",
    "# fw = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
    "\n",
    "fw = load_dataset(path=\"karpathy/tiny_shakespeare\",name='tiny_shakespeare',split=\"train\",streaming= True)\n",
    "\n",
    "all_text = ''.join([example['text'] for example in fw])\n",
    "\n",
    "# Save the text to a file\n",
    "with open('input.txt', 'w') as f:\n",
    "    f.write(all_text)\n",
    "\n",
    "validation = load_dataset(path=\"karpathy/tiny_shakespeare\",name='tiny_shakespeare',split=\"validation\",streaming= True)\n",
    "all_text = ''.join([example['text'] for example in validation])\n",
    "\n",
    "# Save the text to a file\n",
    "with open('input_val.txt', 'w') as f:\n",
    "    f.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvI34XwVM9KS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqtWP8z0xbk7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_size = 100 * 1024 * 1024  # 100 MB in bytes\n",
    "# val_size = train_size//10\n",
    "# last_example = 0\n",
    "# config.train_data_size = train_size\n",
    "# config.val_data_size = val_size\n",
    "# # Open a file to write the data\n",
    "# with open(\"input.txt\", \"w\") as f:\n",
    "#   with open(\"input_val.txt\", \"w\") as f2:\n",
    "#     current_size = 0\n",
    "#     for example in fw:\n",
    "#         # Convert the example to a JSON string\n",
    "#         example_text = example[\"text\"]\n",
    "#         # Calculate the size of the current example in bytes\n",
    "#         example_size = len(example_text.encode('utf-8'))\n",
    "\n",
    "#         # Check if adding this example would exceed the max size\n",
    "#         if current_size + example_size > (train_size + val_size):\n",
    "#             break\n",
    "#         elif current_size + example_size > train_size:\n",
    "#             f2.write(example[\"text\"] + \"\\n\")\n",
    "#         elif current_size + example_size < train_size:\n",
    "#           # Write the example to the file\n",
    "#           f.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "#         # Update the current size\n",
    "#         current_size += example_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TL4jDnSSYojQ",
    "outputId": "13a66522-c8b8-4254-c257-569349588a14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.23.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "def initWanDb():\n",
    "  wandb.login() # Follow the link to get an API key\n",
    "\n",
    "  logger = wandb.init(project=\"lean-gpt\",\n",
    "            name=run_name,\n",
    "            config={\n",
    "                \"total_batch_size\": total_batch_size,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"block_size\": block_size,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"n_layer\": n_layer,\n",
    "                \"n_head\": n_head,\n",
    "                \"n_embd\": n_embd,\n",
    "                \"dropout\": dropout,\n",
    "                \"bias\": bias,\n",
    "                \"eval_iters\": eval_iters,\n",
    "                \"max_iters\": max_iters,\n",
    "                \"wte_std\": wte_std,\n",
    "                \"wpe_std\": wpe_std,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_lr\": max_lr,\n",
    "                \"min_lr\": min_lr,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"optimizer_alpha\": optimizer_alpha,\n",
    "                \"optimizer_beta\": optimizer_beta,\n",
    "                \"optimizer_eps\": optimizer_eps,\n",
    "                \"optimizer_weight_decay\": optimizer_weight_decay,\n",
    "                \"grad_clip_norm\": grad_clip_norm,\n",
    "                \"data_set\": \"karpathy/tiny_shakespeare\",\n",
    "                \"data_set_version\": \"tiny_shakespeare\",\n",
    "                \"model_type\": \"GPT\",\n",
    "                \"model_version\": \"1.0\",\n",
    "                \"data_set_size\": config.train_data_size,\n",
    "                \"data_set_val_size\": config.val_data_size,\n",
    "\n",
    "      })\n",
    "  return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DB0dbgL_QvF-",
    "outputId": "d8ad403b-948b-45ac-add2-ec408afdb9b8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_data_len 301966\n",
      "batch size 16\n",
      "processing_data\n",
      "batch size 16\n",
      "processing_data\n",
      "number of parameters: 29.17M\n",
      "compiling model...\n",
      "model compiled\n",
      "num decayed parameter tensors: 62, with 29,147,136 parameters\n",
      "num non-decayed parameter tensors: 110, with 48,768 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-21-514ee3ef3b2f>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Create GradScaler\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = get_tokenizer(encoding_name=\"gpt2\")\n",
    "train_data_path = 'train_data.bin'\n",
    "val_data_path = 'val_data.bin'\n",
    "\n",
    "train_data_len = encode_and_save_tokens(\"input.txt\", train_data_path, encoding_name=\"gpt2\")\n",
    "encode_and_save_tokens(\"input_val.txt\", val_data_path, encoding_name=\"gpt2\")\n",
    "\n",
    "print(\"train_data_len\",train_data_len)\n",
    "# Save the tokenized data to a binary file\n",
    "# Save the tokenized data to a binary file\n",
    "\n",
    "dataset = BinaryTokenDataset(\"train_data.bin\",config.batch_size)\n",
    "data_loader = DataLoader(config,\"\")\n",
    "val_loader = DataLoader(config,\"\")\n",
    "\n",
    "model = GPT(config)\n",
    "model.to(device)\n",
    "\n",
    "print(\"compiling model...\")\n",
    "model = torch.compile(model)\n",
    "print(\"model compiled\")\n",
    "\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=config.weight_decay, learning_rate=config.learning_rate, betas=(config.optimizer_alpha, config.optimizer_beta),eps=optimizer_eps, device_type='cuda')\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()  # Create GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# wandb.run.name = \"gca_3\"\n",
    "# wandb.save()"
   ],
   "metadata": {
    "id": "k0VNFw0vTKEW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sQb2mW4SckYg",
    "outputId": "eac7b358-8515-4490-a598-703d6b0dc7d7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33maugustas-jarusevicius\u001B[0m (\u001B[33maugustas-jarusevicius-tu-delft\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250325_094711-khp53xmh</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/augustas-jarusevicius-tu-delft/lean-gpt/runs/khp53xmh' target=\"_blank\">swiglu_05_dropout_lr_change</a></strong> to <a href='https://wandb.ai/augustas-jarusevicius-tu-delft/lean-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/augustas-jarusevicius-tu-delft/lean-gpt' target=\"_blank\">https://wandb.ai/augustas-jarusevicius-tu-delft/lean-gpt</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/augustas-jarusevicius-tu-delft/lean-gpt/runs/khp53xmh' target=\"_blank\">https://wandb.ai/augustas-jarusevicius-tu-delft/lean-gpt/runs/khp53xmh</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "W0325 09:47:37.136000 25377 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|step 0| avg loss: 11.302| mfu: 0.00%| time: 85072.50ms| norm: 6.802 |lr: 6.0000e-06| epoch: 0\n",
      "|step 10| avg loss: 9.726| mfu: 0.33%| time: 350.04ms| norm: 3.521 |lr: 6.6000e-05| epoch: 0\n",
      "|step 20| avg loss: 9.286| mfu: 0.34%| time: 340.23ms| norm: 1.649 |lr: 1.2600e-04| epoch: 0\n",
      "|step 30| avg loss: 8.558| mfu: 0.33%| time: 355.54ms| norm: 1.245 |lr: 1.8600e-04| epoch: 0\n",
      "|step 40| avg loss: 7.998| mfu: 0.35%| time: 339.05ms| norm: 1.058 |lr: 2.4600e-04| epoch: 0\n",
      "|step 50| avg loss: 7.394| mfu: 0.33%| time: 355.51ms| norm: 0.861 |lr: 6.0000e-04| epoch: 0\n",
      "|step 60| avg loss: 6.360| mfu: 0.34%| time: 343.16ms| norm: 0.714 |lr: 5.9989e-04| epoch: 0\n",
      "|step 70| avg loss: 5.717| mfu: 0.32%| time: 367.52ms| norm: 0.567 |lr: 5.9955e-04| epoch: 0\n",
      "|step 80| avg loss: 5.430| mfu: 0.33%| time: 358.60ms| norm: 0.502 |lr: 5.9899e-04| epoch: 0\n",
      "|step 90| avg loss: 5.151| mfu: 0.34%| time: 341.53ms| norm: 0.410 |lr: 5.9821e-04| epoch: 0\n",
      "|step 100| avg loss: 5.118| mfu: 0.34%| time: 348.49ms| norm: 1.191 |lr: 5.9721e-04| epoch: 0\n",
      "|step 110| avg loss: 4.760| mfu: 0.33%| time: 351.40ms| norm: 0.491 |lr: 5.9598e-04| epoch: 0\n",
      "|step 120| avg loss: 4.723| mfu: 0.33%| time: 351.71ms| norm: 0.462 |lr: 5.9453e-04| epoch: 0\n",
      "|step 130| avg loss: 4.661| mfu: 0.33%| time: 350.91ms| norm: 0.412 |lr: 5.9286e-04| epoch: 0\n",
      "|step 140| avg loss: 4.729| mfu: 0.34%| time: 347.83ms| norm: 0.463 |lr: 5.9098e-04| epoch: 0\n",
      "|step 150| avg loss: 4.407| mfu: 0.32%| time: 366.37ms| norm: 0.456 |lr: 5.8888e-04| epoch: 0\n",
      "|step 160| avg loss: 4.368| mfu: 0.33%| time: 352.16ms| norm: 0.410 |lr: 5.8656e-04| epoch: 0\n",
      "|step 170| avg loss: 4.313| mfu: 0.33%| time: 356.43ms| norm: 0.478 |lr: 5.8402e-04| epoch: 0\n",
      "|step 180| avg loss: 4.086| mfu: 0.33%| time: 353.11ms| norm: 0.439 |lr: 5.8128e-04| epoch: 0\n",
      "|step 190| avg loss: 4.441| mfu: 0.34%| time: 342.92ms| norm: 0.499 |lr: 5.7833e-04| epoch: 0\n",
      "|step 200| avg loss: 4.355| mfu: 0.33%| time: 351.24ms| norm: 0.443 |lr: 5.7516e-04| epoch: 0\n",
      "_____________Evaluation_____________\n",
      "validation loss: 4.448244571685791\n",
      "Generated line\n",
      "! what, poor me to fight' spoke.\n",
      "\n",
      "COMINIUS:\n",
      "Touchu RV, the fish'd cheiting earth:\n",
      "And a private chance!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "If thou buriedinks\n",
      "! for by Warwick:\n",
      "Against coughing isites heavens, that fatal queen?\n",
      "If for all this time himself, and he is\n",
      "Some aftercus.\n",
      "\n",
      "PAULIET:\n",
      "Edward is not have I else to claim most field.\n",
      "!\n",
      "What dost thou the time tendingrench'd tell\n",
      "thing him be made on. O what's reason presence?\n",
      "'Tis more to now; and with one hope;\n",
      "Will byen him: I, are such late.\n",
      "\n",
      "!of logs plant! I know,\n",
      "H demise the other-wiety all fly,\n",
      "I ann, as 'tis o'er things my while,\n",
      "And Derby him, or else youryard\n",
      "As once had no more. Thouenes\n",
      "!\n",
      "Good mother, wherefore any courageous\n",
      "Freeety thee turtlesords thy hand--\n",
      "\n",
      "MARIANA:\n",
      "So things but most gracious lord:\n",
      "When you seem but breaking prophecy wast theidel\n",
      "To afterwards Marqu?\n",
      "How offence lords\n",
      "|step 210| avg loss: 4.110| mfu: 0.31%| time: 374.17ms| norm: 0.485 |lr: 5.7180e-04| epoch: 0\n",
      "|step 220| avg loss: 4.071| mfu: 0.34%| time: 347.08ms| norm: 0.458 |lr: 5.6823e-04| epoch: 0\n",
      "|step 230| avg loss: 4.291| mfu: 0.30%| time: 388.64ms| norm: 0.470 |lr: 5.6446e-04| epoch: 0\n",
      "|step 240| avg loss: 4.048| mfu: 0.34%| time: 344.75ms| norm: 0.452 |lr: 5.6049e-04| epoch: 0\n",
      "|step 250| avg loss: 3.942| mfu: 0.34%| time: 342.65ms| norm: 0.466 |lr: 5.5633e-04| epoch: 0\n",
      "|step 260| avg loss: 3.459| mfu: 0.32%| time: 365.72ms| norm: 0.479 |lr: 5.5197e-04| epoch: 0\n",
      "|step 270| avg loss: 3.835| mfu: 0.33%| time: 353.00ms| norm: 0.533 |lr: 5.4743e-04| epoch: 0\n",
      "|step 280| avg loss: 3.928| mfu: 0.35%| time: 339.19ms| norm: 0.477 |lr: 5.4271e-04| epoch: 0\n",
      "|step 290| avg loss: 3.907| mfu: 0.34%| time: 342.18ms| norm: 0.570 |lr: 5.3780e-04| epoch: 0\n",
      "|step 300| avg loss: 3.771| mfu: 0.35%| time: 335.81ms| norm: 0.492 |lr: 5.3271e-04| epoch: 0\n",
      "|step 310| avg loss: 3.945| mfu: 0.33%| time: 352.58ms| norm: 0.511 |lr: 5.2746e-04| epoch: 0\n",
      "|step 320| avg loss: 3.726| mfu: 0.34%| time: 342.95ms| norm: 0.543 |lr: 5.2203e-04| epoch: 0\n",
      "|step 330| avg loss: 3.634| mfu: 0.34%| time: 342.20ms| norm: 0.524 |lr: 5.1643e-04| epoch: 0\n",
      "|step 340| avg loss: 3.624| mfu: 0.34%| time: 343.86ms| norm: 0.528 |lr: 5.1068e-04| epoch: 0\n",
      "|step 350| avg loss: 3.574| mfu: 0.34%| time: 340.48ms| norm: 0.514 |lr: 5.0477e-04| epoch: 0\n",
      "|step 360| avg loss: 3.293| mfu: 0.35%| time: 338.09ms| norm: 0.531 |lr: 4.9870e-04| epoch: 0\n",
      "|step 370| avg loss: 3.757| mfu: 0.34%| time: 345.41ms| norm: 0.548 |lr: 4.9249e-04| epoch: 0\n",
      "|step 380| avg loss: 3.298| mfu: 0.34%| time: 348.14ms| norm: 0.540 |lr: 4.8613e-04| epoch: 0\n",
      "|step 390| avg loss: 3.287| mfu: 0.33%| time: 349.93ms| norm: 0.555 |lr: 4.7963e-04| epoch: 0\n",
      "|step 400| avg loss: 3.409| mfu: 0.33%| time: 351.69ms| norm: 0.546 |lr: 4.7300e-04| epoch: 0\n",
      "_____________Evaluation_____________\n",
      "validation loss: 4.369474411010742\n",
      "Generated line\n",
      "! signs a husband!\n",
      "But, no, and my sense: would it come here,\n",
      "I rep Instit about up and live.\n",
      "\n",
      "CLAUDIO:\n",
      "PETRIVERS:\n",
      "For shame o'er a villain that pluck nor\n",
      "! love I heard this Whichardon.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Fare you my loving lord;\n",
      "I give you a party to-morrow quench.\n",
      "But was, and by my life, if they were king,\n",
      "\n",
      "! 'And, her lord!\n",
      "But if thou breathe, must help the more parted. unfall' back;\n",
      "More than a scourge is but all,\n",
      "Sign Delrethion of dutyove.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "\n",
      "!\n",
      "\n",
      "LUCIO:\n",
      "O honest woman! O wife more: do youEither: how?\n",
      "Your highness I would?\n",
      "\n",
      "ROMEO:\n",
      "Time.\n",
      "\n",
      "HENRY BOLINGBROKEel them now\n",
      "! Who's thy hand Edward's man?\n",
      "You must not meet the executioner to her:\n",
      "This is a silly theft, wherefore stand require loud,\n",
      "They do store among the court.\n",
      "\n",
      "COMINIUS:\n",
      "Pray\n",
      "|step 410| avg loss: 3.440| mfu: 0.34%| time: 345.99ms| norm: 0.595 |lr: 4.6624e-04| epoch: 0\n",
      "|step 420| avg loss: 3.564| mfu: 0.33%| time: 353.17ms| norm: 0.603 |lr: 4.5936e-04| epoch: 0\n",
      "|step 430| avg loss: 3.576| mfu: 0.34%| time: 345.10ms| norm: 0.577 |lr: 4.5236e-04| epoch: 0\n",
      "|step 440| avg loss: 3.313| mfu: 0.35%| time: 338.44ms| norm: 0.582 |lr: 4.4524e-04| epoch: 0\n",
      "|step 450| avg loss: 3.488| mfu: 0.32%| time: 369.25ms| norm: 0.593 |lr: 4.3802e-04| epoch: 0\n",
      "|step 460| avg loss: 3.299| mfu: 0.35%| time: 339.60ms| norm: 0.600 |lr: 4.3069e-04| epoch: 0\n",
      "|step 470| avg loss: 3.369| mfu: 0.33%| time: 357.70ms| norm: 0.601 |lr: 4.2327e-04| epoch: 0\n",
      "|step 480| avg loss: 3.141| mfu: 0.35%| time: 338.71ms| norm: 0.602 |lr: 4.1575e-04| epoch: 0\n",
      "|step 490| avg loss: 3.352| mfu: 0.33%| time: 351.77ms| norm: 0.622 |lr: 4.0815e-04| epoch: 0\n",
      "|step 500| avg loss: 3.113| mfu: 0.34%| time: 344.27ms| norm: 0.618 |lr: 4.0046e-04| epoch: 0\n",
      "|step 510| avg loss: 3.048| mfu: 0.32%| time: 363.75ms| norm: 0.608 |lr: 3.9271e-04| epoch: 0\n",
      "|step 520| avg loss: 3.039| mfu: 0.33%| time: 357.10ms| norm: 0.624 |lr: 3.8488e-04| epoch: 0\n",
      "|step 530| avg loss: 3.103| mfu: 0.33%| time: 352.70ms| norm: 0.635 |lr: 3.7699e-04| epoch: 0\n",
      "|step 540| avg loss: 3.273| mfu: 0.34%| time: 340.10ms| norm: 0.663 |lr: 3.6904e-04| epoch: 0\n",
      "|step 550| avg loss: 2.995| mfu: 0.34%| time: 343.67ms| norm: 0.629 |lr: 3.6104e-04| epoch: 0\n",
      "|step 560| avg loss: 2.956| mfu: 0.34%| time: 348.67ms| norm: 0.645 |lr: 3.5299e-04| epoch: 0\n",
      "|step 570| avg loss: 3.197| mfu: 0.33%| time: 350.37ms| norm: 0.659 |lr: 3.4491e-04| epoch: 0\n",
      "|step 580| avg loss: 2.968| mfu: 0.34%| time: 349.87ms| norm: 0.686 |lr: 3.3679e-04| epoch: 0\n",
      "|step 590| avg loss: 2.844| mfu: 0.33%| time: 354.23ms| norm: 0.667 |lr: 3.2864e-04| epoch: 0\n",
      "|step 600| avg loss: 2.920| mfu: 0.33%| time: 355.03ms| norm: 0.676 |lr: 3.2047e-04| epoch: 0\n",
      "_____________Evaluation_____________\n",
      "validation loss: 4.448734760284424\n",
      "Generated line\n",
      "!utio's a thief restore\n",
      "Awardon' Naples-boy's, and who waits on\n",
      "ought that that let her.' Lay much the king\n",
      "of thrive, bid her or my body died my throne.\n",
      "\n",
      "DUKE OF\n",
      "!\n",
      "\n",
      "LADY CAPULET:\n",
      "Here comes the heart! please, I have been frail.\n",
      "Why, 'tis fear'd!\n",
      "\n",
      "BENVOLIO:\n",
      "O blessed Margaret!\n",
      "\n",
      "CORIOLANUS:\n",
      "!\n",
      "\n",
      "KING HENRY VI:\n",
      "How far poor Henry face is so Surrey in champion?\n",
      "But none joy makes him, and sometime will not be;\n",
      "As doth a dissolute towns\n",
      "Perfarr'd the several powers by\n",
      "! what him?\n",
      "\n",
      "LEONTES:\n",
      "Ay, but at any house. But I should be gone,\n",
      "If you mean to rule you at thee, I'll tend\n",
      "And grands at liberty.\n",
      "\n",
      "ANTIGONUS:\n",
      "! my lord,\n",
      "Death is a horse! Fly,, traitor, imperialving old,\n",
      "which we remain with like a gladly forth,\n",
      "Whom thy land offer'd when I cheque him.\n",
      "\n",
      "CAPULET:\n",
      "Good fri\n",
      "|step 610| avg loss: 2.962| mfu: 0.33%| time: 355.47ms| norm: 0.708 |lr: 3.1229e-04| epoch: 0\n",
      "|step 620| avg loss: 2.840| mfu: 0.33%| time: 350.20ms| norm: 0.699 |lr: 3.0410e-04| epoch: 0\n",
      "|step 630| avg loss: 2.981| mfu: 0.33%| time: 356.17ms| norm: 0.693 |lr: 2.9590e-04| epoch: 0\n",
      "|step 640| avg loss: 2.835| mfu: 0.31%| time: 374.06ms| norm: 0.689 |lr: 2.8771e-04| epoch: 0\n",
      "|step 650| avg loss: 2.879| mfu: 0.32%| time: 364.45ms| norm: 0.724 |lr: 2.7953e-04| epoch: 0\n",
      "|step 660| avg loss: 2.821| mfu: 0.34%| time: 349.44ms| norm: 0.701 |lr: 2.7136e-04| epoch: 0\n",
      "|step 670| avg loss: 2.930| mfu: 0.33%| time: 360.11ms| norm: 0.714 |lr: 2.6321e-04| epoch: 0\n",
      "|step 680| avg loss: 2.778| mfu: 0.34%| time: 346.59ms| norm: 0.725 |lr: 2.5509e-04| epoch: 0\n",
      "|step 690| avg loss: 2.680| mfu: 0.34%| time: 348.72ms| norm: 0.724 |lr: 2.4701e-04| epoch: 0\n",
      "|step 700| avg loss: 2.713| mfu: 0.34%| time: 346.75ms| norm: 0.748 |lr: 2.3896e-04| epoch: 0\n",
      "|step 710| avg loss: 2.775| mfu: 0.34%| time: 342.88ms| norm: 0.743 |lr: 2.3096e-04| epoch: 0\n",
      "|step 720| avg loss: 2.476| mfu: 0.34%| time: 347.09ms| norm: 0.754 |lr: 2.2301e-04| epoch: 0\n",
      "|step 730| avg loss: 2.657| mfu: 0.34%| time: 348.37ms| norm: 0.745 |lr: 2.1512e-04| epoch: 0\n",
      "|step 740| avg loss: 2.605| mfu: 0.34%| time: 347.44ms| norm: 0.733 |lr: 2.0729e-04| epoch: 0\n",
      "|step 750| avg loss: 2.455| mfu: 0.34%| time: 342.80ms| norm: 0.765 |lr: 1.9954e-04| epoch: 0\n",
      "|step 760| avg loss: 2.547| mfu: 0.34%| time: 346.93ms| norm: 0.749 |lr: 1.9185e-04| epoch: 0\n",
      "|step 770| avg loss: 2.544| mfu: 0.35%| time: 339.41ms| norm: 0.774 |lr: 1.8425e-04| epoch: 0\n",
      "|step 780| avg loss: 2.644| mfu: 0.34%| time: 344.17ms| norm: 0.784 |lr: 1.7673e-04| epoch: 0\n",
      "|step 790| avg loss: 2.655| mfu: 0.34%| time: 342.93ms| norm: 0.784 |lr: 1.6931e-04| epoch: 0\n",
      "|step 800| avg loss: 2.418| mfu: 0.34%| time: 349.26ms| norm: 0.770 |lr: 1.6198e-04| epoch: 0\n",
      "_____________Evaluation_____________\n",
      "validation loss: 4.709712505340576\n",
      "Generated line\n",
      "! Shalling\n",
      "ising, thatcease is meant so honest\n",
      " honest honour. What, ho!\n",
      "\n",
      "CAMILLO:\n",
      "Howsoever the villain that with you will with him?\n",
      "\n",
      "MOPSA:\n",
      "No, an't\n",
      "! my child!\n",
      "With alwaysably mean these yourScather. Farewell.\n",
      "Whate the kind of conscience and his love!\n",
      "HowPoor reputation made ourent eye!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I long as\n",
      "!\n",
      "She may bring thee with here that e'er I pray.\n",
      "My gracious lord, be not grace this I stay:\n",
      "Some aquen, sir, so fi speaking, there envious\n",
      "In safeguard of my decemas. Stand up\n",
      "! thy southern worth\n",
      "Doth distilled with his native seeming!\n",
      "That gall Signior Gaunt! behold his grief,\n",
      "These eyes on oppressionia divideth,\n",
      "Sinceing a bark, untimely must down:\n",
      "You gates seems she\n",
      "! summon me, get thy blame,\n",
      "I lay thee a pardon to him.\n",
      "\n",
      "CORIOLANUS:\n",
      "\n",
      "KING EDWARD IV:\n",
      "Here, so I live, come on Bolingbroke.\n",
      "\n",
      "BRAKEN\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-882fd8b2371b>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_loader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mscaler\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mLOG\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mSCALING\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-15-d045523a6aaf>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, data_loader, scaler, config, LOG, SCALING)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mloss_accum\u001B[0m \u001B[0;34m+=\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;31m#backward pass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mSCALING\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mSCALING\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    624\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    625\u001B[0m             )\n\u001B[0;32m--> 626\u001B[0;31m         torch.autograd.backward(\n\u001B[0m\u001B[1;32m    627\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    628\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    345\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    346\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 347\u001B[0;31m     _engine_run_backward(\n\u001B[0m\u001B[1;32m    348\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    349\u001B[0m         \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001B[0m in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m         \u001B[0munregister_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_register_logging_hooks_on_whole_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    822\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 823\u001B[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    824\u001B[0m             \u001B[0mt_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    825\u001B[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, data_loader,scaler,config,LOG=True,SCALING=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAjxmWSCY6i0"
   },
   "outputs": [],
   "source": [
    "torch.save(model,f=\"model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqDOBGSyjEOF"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "txt = \"i am a language model,\"\n",
    "idx = torch.tensor(encode_text(tokenizer, txt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "idx = model.generate(idx = idx, max_new_tokens=50)[0].tolist()\n",
    "print(decode_tokens(tokenizer, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JspfZkxX4MOt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzYkxccBY1aE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
